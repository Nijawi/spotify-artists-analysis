---
title: "Spotify Artists Analysis"
author: "James Le"
date: 'Updated: `r Sys.Date()`'
output:
  html_document:
    df_print: paged
    toc: yes
    code_folding: hide
    number_sections: yes
---

# Introduction

# Getting Data

The first step is registering my application in the [API Website](https://beta.developer.spotify.com/web-api/) and getting the keys (Client ID and Client Secret) for future requests.

The Spotify Web API has different URI (Uniform Resource Identifier) to access playlists, artists or tracks information. Consequently, the process of getting data must be divided in 2 key steps.

* Get the "This Is" Playlist Series for multiple musicians.
* Get the audio features for each artist´s Playlist tracks.

## Web API Credentials

First, I created two variables for the *Client ID* and the *Client Secret* credentials.

```{r}
spotifyKey <- "182878ec396d424283c951d6769e9497"
spotifySecret <- "2a6d8f846edc4667ba9f0ba43cd7fe4c"
```

After that, I requested an access token in order to authorise my app to retrieve and manage Spotify data.

```{r}
library(Rspotify)
library(httr)
library(jsonlite)

spotifyEndpoint <- oauth_endpoint(NULL, 
                                  "https://accounts.spotify.com/authorize",
                                  "https://accounts.spotify.com/api/token")
spotifyToken <- spotifyOAuth("Spotify Analysis", spotifyKey, spotifySecret)
```

## "This Is" Series Playlist

The first step to pull the artists´ ["This Is" series](https://open.spotify.com/search/playlists/this%20is%20) is to get the URI´s for each one. I stored each musician´s This Is Playlist URI in a .csv file and imported it in R.

```{r}
library(readr)
playlistURI <- read.csv("this-is-playlist-URI.csv", header = T, sep = ";")
```

With each Playlist URI, I applied the *getPlaylistSongs* from the *RSpotify* package and stored the Playlist information in an empty dataframe.

```{r}
# Empty dataframe
PlaylistSongs <- data.frame(PlaylistID = character(),
                            Musician = character(),
                            tracks = character(),
                            id = character(),
                            popularity = integer(),
                            artist = character(),
                            artistId = character(),
                            album = character(),
                            albumId = character(),
                            stringsAsFactors=FALSE) 
```

```{r}
# Getting each playlist
for (i in 1:nrow(playlistURI)) {
  i <- cbind(PlaylistID = as.factor(playlistURI[i,2]),
             Musician = as.factor(playlistURI[i,1]),
             getPlaylistSongs("spotify",
                              playlistid = as.factor(playlistURI[i,2]),
                              token=spotifyToken))
  PlaylistSongs <- rbind(PlaylistSongs, i)
}
```

As we can see below, the dataframe has 10 columns and 2129 rows.

```{r}
dim(PlaylistSongs)
```

The following table shows the first 86 rows of my dataframe PlaylistSongs. It contains the tracks by Taylor Swift and Ariana Grande.

```{r}
library(knitr)
library(kableExtra)
library(dplyr)
options(knitr.table.format = "html")
options(width = 12)

# Only Taylor Swift and Ariana Grande
kable(head(PlaylistSongs,86)) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 12) %>%
scroll_box(width = "1000px", height = "750px")
```

## Audio Features

First, I wrote a formula (*getFeatures*) that extracts the audio features for any specific ID stored as a vector.

```{r}
getFeatures <- function (vector_id, token) 
{
  link <- httr::GET(paste0("https://api.spotify.com/v1/audio-features/?ids=", 
  vector_id), httr::config(token = token))
  list <- httr::content(link)
  return(list)
}
```

Next, I included *getFeatures* in another formula (*get_features*). The latter formula extracts the audio features for the track ID’s vector and returns them in a dataframe.

```{r}
get_features <- function (x) 
{
  getFeatures2 <- getFeatures(vector_id = x, token = spotifyToken)
  features_output <- do.call(rbind, lapply(getFeatures2$audio_features, data.frame, stringsAsFactors=FALSE))
}
```

Using the formula created above, I will be able to extract the audio features for each track. In order to do so, I need a vector containing each track ID. The rate limit for the Spotify API is 100 tracks, so I decided to create a vector with track IDs for each musician.

```{r}
TaylorSwift_vc <- paste(as.character(PlaylistSongs$id[1:38]), sep="", collapse=",")
ArianaGrande_vc <- paste(as.character(PlaylistSongs$id[39:86]), sep="", collapse=",")
KendrickLamar_vc <- paste(as.character(PlaylistSongs$id[87:124]), sep="", collapse=",")
ShawnMendes_vc <- paste(as.character(PlaylistSongs$id[125:177]), sep="", collapse=",")
Maroon5_vc <- paste(as.character(PlaylistSongs$id[178:226]), sep="", collapse=",")
PostMalone_vc <- paste(as.character(PlaylistSongs$id[227:261]), sep="", collapse=",")
Kygo_vc <- paste(as.character(PlaylistSongs$id[262:299]), sep="", collapse=",")
TheChainsmokers_vc <- paste(as.character(PlaylistSongs$id[300:333]), sep="", collapse=",")
Adele_vc <- paste(as.character(PlaylistSongs$id[334:358]), sep="", collapse=",")
Drake_vc <- paste(as.character(PlaylistSongs$id[359:408]), sep="", collapse=",")
JustinBieber_vc <- paste(as.character(PlaylistSongs$id[409:457]), sep="", collapse=",")
Coldplay_vc <- paste(as.character(PlaylistSongs$id[458:494]), sep="",collapse=",")
KanyeWest_vc <- paste(as.character(PlaylistSongs$id[495:545]), sep="", collapse=",")
BrunoMars_vc <- paste(as.character(PlaylistSongs$id[546:584]), sep="", collapse=",")
EdSheeran_vc <- paste(as.character(PlaylistSongs$id[585:624]), sep="", collapse=",")
Eminem_vc <- paste(as.character(PlaylistSongs$id[625:679]), sep="", collapse=",")
Beyonce_vc <- paste(as.character(PlaylistSongs$id[680:711]), sep="", collapse=",")
Avicii_vc <- paste(as.character(PlaylistSongs$id[712:770]), sep="", collapse=",")
Marshmello_vc <- paste(as.character(PlaylistSongs$id[771:808]), sep="", collapse=",")
CalvinHarris_vc <- paste(as.character(PlaylistSongs$id[809:846]), sep="", collapse=",")
JustinTimberlake_vc <- paste(as.character(PlaylistSongs$id[847:912]), sep="", collapse=",")
FrankSinatra_vc <- paste(as.character(PlaylistSongs$id[913:962]), sep="", collapse=",")
CharliePuth_vc <- paste(as.character(PlaylistSongs$id[963:993]), sep="", collapse=",")
MichaelBuble_vc <- paste(as.character(PlaylistSongs$id[994:1035]), sep="", collapse=",")
MartinGarrix_vc <- paste(as.character(PlaylistSongs$id[1036:1084]), sep="", collapse=",")
EnriqueIglesias_vc <- paste(as.character(PlaylistSongs$id[1085:1125]), sep="", collapse=",")
JohnMayer_vc <- paste(as.character(PlaylistSongs$id[1126:1184]), sep="", collapse=",")
Future_vc <- paste(as.character(PlaylistSongs$id[1185:1224]), sep="", collapse=",")
EltonJohn_vc <- paste(as.character(PlaylistSongs$id[1225:1265]), sep="", collapse=",")
FiftyCent_vc <- paste(as.character(PlaylistSongs$id[1266:1315]), sep="", collapse=",")
Lorde_vc <- paste(as.character(PlaylistSongs$id[1316:1346]), sep="", collapse=",")
LilWayne_vc <- paste(as.character(PlaylistSongs$id[1347:1396]), sep="", collapse=",")
WizKhalifa_vc <- paste(as.character(PlaylistSongs$id[1397:1446]), sep="", collapse=",")
FifthHarmony_vc <- paste(as.character(PlaylistSongs$id[1447:1479]), sep="", collapse=",")
LanaDelRay_vc <- paste(as.character(PlaylistSongs$id[1480:1524]), sep="",collapse=",")
NorahJones_vc <- paste(as.character(PlaylistSongs$id[1525:1562]), sep="", collapse=",")
JamesArthur_vc <- paste(as.character(PlaylistSongs$id[1563:1581]), sep="", collapse=",")
OneRepublic_vc <- paste(as.character(PlaylistSongs$id[1582:1614]), sep="", collapse=",")
TheScript_vc <- paste(as.character(PlaylistSongs$id[1615:1658]), sep="", collapse=",")
StevieWonder_vc <- paste(as.character(PlaylistSongs$id[1659:1708]), sep="", collapse=",")
JasonMraz_vc <- paste(as.character(PlaylistSongs$id[1709:1758]), sep="", collapse=",")
JohnLegend_vc <- paste(as.character(PlaylistSongs$id[1759:1795]), sep="", collapse=",")
Pentatonix_vc <- paste(as.character(PlaylistSongs$id[1796:1834]), sep="", collapse=",")
AliciaKeys_vc <- paste(as.character(PlaylistSongs$id[1835:1884]), sep="", collapse=",")
Usher_vc <- paste(as.character(PlaylistSongs$id[1885:1934]), sep="", collapse=",")
SnoopDogg_vc <- paste(as.character(PlaylistSongs$id[1935:1984]), sep="", collapse=",")
Macklemore_vc <- paste(as.character(PlaylistSongs$id[1985:2007]), sep="",collapse=",")
ZaraLarsson_vc <- paste(as.character(PlaylistSongs$id[2008:2043]), sep="", collapse=",")
JayZ_vc <- paste(as.character(PlaylistSongs$id[2044:2093]), sep="", collapse=",")
Rihanna_vc <- paste(as.character(PlaylistSongs$id[2094:2129]), sep="", collapse=",")
```

Next, I apply the *get_features* formula to each vector obtaining the audio features for each musician.

```{r}
TaylorSwift <- get_features(TaylorSwift_vc)
ArianaGrande <- get_features(ArianaGrande_vc)
KendrickLamar <- get_features(KendrickLamar_vc)
ShawnMendes <- get_features(ShawnMendes_vc)
Maroon5 <- get_features(Maroon5_vc)
PostMalone <- get_features(PostMalone_vc)
Kygo <- get_features(Kygo_vc)
TheChainsmokers <- get_features(TheChainsmokers_vc)
Adele <- get_features(Adele_vc)
Drake <- get_features(Drake_vc)
JustinBieber <- get_features(JustinBieber_vc)
Coldplay <- get_features(Coldplay_vc)
KanyeWest <- get_features(KanyeWest_vc)
BrunoMars <- get_features(BrunoMars_vc)
EdSheeran <- get_features(EdSheeran_vc)
Eminem <- get_features(Eminem_vc)
Beyonce <- get_features(Beyonce_vc)
Avicii <- get_features(Avicii_vc)
Marshmello <- get_features(Marshmello_vc)
CalvinHarris <- get_features(CalvinHarris_vc)
JustinTimberlake <- get_features(JustinTimberlake_vc)
FrankSinatra <- get_features(FrankSinatra_vc)
CharliePuth <- get_features(CharliePuth_vc)
MichaelBuble <- get_features(MichaelBuble_vc)
MartinGarrix <- get_features(MartinGarrix_vc)
EnriqueIglesias <- get_features(EnriqueIglesias_vc)
JohnMayer <- get_features(JohnMayer_vc)
Future <- get_features(Future_vc)
EltonJohn <- get_features(EltonJohn_vc)
FiftyCent <- get_features(FiftyCent_vc)
Lorde <- get_features(Lorde_vc)
LilWayne <- get_features(LilWayne_vc)
WizKhalifa <- get_features(WizKhalifa_vc)
FifthHarmony <- get_features(FifthHarmony_vc)
LanaDelRay <- get_features(LanaDelRay_vc)
NorahJones <- get_features(NorahJones_vc)
JamesArthur <- get_features(JamesArthur_vc)
OneRepublic <- get_features(OneRepublic_vc)
TheScript <- get_features(TheScript_vc)
StevieWonder <- get_features(StevieWonder_vc)
JasonMraz <- get_features(JasonMraz_vc)
JohnLegend <- get_features(JohnLegend_vc)
Pentatonix <- get_features(Pentatonix_vc)
AliciaKeys <- get_features(AliciaKeys_vc)
Usher <- get_features(Usher_vc)
SnoopDogg <- get_features(SnoopDogg_vc)
Macklemore <- get_features(Macklemore_vc)
ZaraLarsson <- get_features(ZaraLarsson_vc)
JayZ <- get_features(JayZ_vc)
Rihanna <- get_features(Rihanna_vc)
```

After that, I merged each musician´s audio features dataframe into a new one, *all_features*. It contains the audio features for all the tracks in every musician's This Is Playlist.

```{r}
library(gdata)
all_features <- combine(TaylorSwift,ArianaGrande,KendrickLamar,ShawnMendes,Maroon5,PostMalone,Kygo,TheChainsmokers,Adele,Drake,JustinBieber,Coldplay,KanyeWest,BrunoMars,EdSheeran,Eminem,Beyonce,Avicii,Marshmello,CalvinHarris,JustinTimberlake,FrankSinatra,CharliePuth,MichaelBuble,MartinGarrix,EnriqueIglesias,JohnMayer,Future,EltonJohn,FiftyCent,Lorde,LilWayne,WizKhalifa,FifthHarmony,LanaDelRay,NorahJones,JamesArthur,OneRepublic,TheScript,StevieWonder,JasonMraz,JohnLegend,Pentatonix,AliciaKeys,Usher,SnoopDogg,Macklemore,ZaraLarsson,JayZ,Rihanna)
```

A preview of the *all_features* dataframe can be found below. It only shows 86 rows with the data for Taylor Swift and Ariana Grande. The last column (*Source*) contains the musician.

```{r}
options(knitr.table.format = "html")
options(width = 12)

kable(head(all_features, 86)) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 12) %>%
scroll_box(width = "1000px", height = "750px")
```

Finally, I have computed the mean of each musician´s audio features using the *aggregate* function. The resulting dataframe contains the audio features for each musician as the mean of the tracks in their respective playlists.

```{r}
mean_features <- aggregate(all_features[, c(1:11,17)], list(all_features$source), mean)
names(mean_features) <- c("Musician", "danceability", "energy", "key", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo", "duration_ms")
```

```{r}
options(knitr.table.format = "html")
options(width = 12)
kable(mean_features) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 12) %>%
scroll_box(width = "1000px", height = "500px")
```

## Audio Features Description

The description of each feature from the [Spotify Web API Guidance](https://beta.developer.spotify.com/web-api/get-audio-features/) can be found below:

* **Danceability**: Describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.

* **Energy**: Is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.

* **Key**: The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on.

* **Loudness**: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.

* **Mode**: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.

* **Speechiness**: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.

* **Acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.

* **Instrumentalness**: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.

* **Liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.

* **Valence**: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

* **Tempo**: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.

* **Duration_ms**: The duration of the track in milliseconds.

# Data Visualization

## Radar Chart

A radar chart is useful to compare the musical vibes of these musicians in a more visual way. The first visualisation is an R implementation of the radar chart from the [chart.js](http://www.chartjs.org/) javascript library and evaluates the audio features for 10 selected musicians.

First, I normalised the values to be from 0 to 1. This will help to make the chart more clear and readable.

```{r}
mean_features_norm <- cbind(mean_features[1], 
                                     apply(mean_features[-1],2,
                                           function(x){(x-min(x)) / diff(range(x))}))
```

Next, an interactive radar chart can be plotted. It displays data set labels when hovering over each radial line, showing the value for the selected feature.

```{r}
library(radarchart)
library(tidyr)
sample1 <- mean_features[mean_features$Musician %in% c("TaylorSwift", "ArianaGrande", "KendrickLamar", "ShawnMendes", "Maroon5", "PostMalone", "Kygo", "TheChainsmokers", "Adele", "Drake"),]

mean_features_norm_1 <- cbind(sample1[1], 
                                        apply(sample1[-1],2,
                                              function(x){(x-min(x)) / diff(range(x))})) 

radarDF_1 <- gather(mean_features_norm_1, key=Attribute, value=Score, -Musician) %>%
  spread(key=Musician, value=Score)

chartJSRadar(scores = radarDF_1,
             scaleStartValue = -1, 
             maxScale =1, 
             showToolTipLabel = TRUE)
```

```{r}
sample2 <- mean_features[mean_features$Musician %in% c("JustinBieber", "Coldplay", "KanyeWest", "BrunoMars", "EdSheeran", "Eminem", "Beyonce", "Avicii", "Marshmello", "CalvinHarris"),]

mean_features_norm_2 <- cbind(sample2[1], apply(sample2[-1],2,function(x){(x-min(x)) / diff(range(x))})) 

radarDF_2 <- gather(mean_features_norm_2, key=Attribute, value=Score, -Musician) %>%
  spread(key=Musician, value=Score)

chartJSRadar(scores = radarDF_2, scaleStartValue = -1, maxScale = 1, showToolTipLabel = TRUE)
```

```{r}
sample3 <- mean_features[mean_features$Musician %in% c("JustinTimberlake", "FrankSinatra", "CharliePuth", "MichaelBuble", "MartinGarrix", "EnriqueIglesias", "JohnMayer", "Future", "EltonJohn", "FiftyCent"),]

mean_features_norm_3 <- cbind(sample3[1], apply(sample3[-1],2,function(x){(x-min(x)) / diff(range(x))})) 

radarDF_3 <- gather(mean_features_norm_3, key=Attribute, value=Score, -Musician) %>%
  spread(key=Musician, value=Score)

chartJSRadar(scores = radarDF_3, scaleStartValue = -1, maxScale = 1, showToolTipLabel = TRUE)
```

```{r}
sample4 <- mean_features[mean_features$Musician %in% c("Lorde", "LilWayne", "WizKhalifa", "FifthHarmony", "LanaDelRay", "NorahJones", "JamesArthur", "OneRepublic", "TheScript", "StevieWonder"),]

mean_features_norm_4 <- cbind(sample4[1], apply(sample4[-1],2,function(x){(x-min(x)) / diff(range(x))})) 

radarDF_4 <- gather(mean_features_norm_4, key=Attribute, value=Score, -Musician) %>%
  spread(key=Musician, value=Score)

chartJSRadar(scores = radarDF_4, scaleStartValue = -1, maxScale = 1, showToolTipLabel = TRUE)
```

```{r}
sample5 <- mean_features[mean_features$Musician %in% c("JasonMraz", "JohnLegend", "Pentatonix", "AliciaKeys", "Usher", "SnoopDogg", "Macklemore", "ZaraLarsson", "JayZ", "Rihanna"),]

mean_features_norm_5 <- cbind(sample5[1], apply(sample5[-1],2,function(x){(x-min(x)) / diff(range(x))})) 

radarDF_5 <- gather(mean_features_norm_5, key=Attribute, value=Score, -Musician) %>%
  spread(key=Musician, value=Score)

chartJSRadar(scores = radarDF_5, scaleStartValue = -1, maxScale = 1, showToolTipLabel = TRUE)
```

## Cluster Analysis

Another way to find out the differences between these musicians in their musical repertoire is grouping them in clusters. The general idea of a clustering algorithm is to divide a given dataset into multiple groups on the basis of similarity in the data. In this case, musicians will be grouped in different clusters according to their music preferences.

Prior to clustering data, it is important to rescale the numeric variables of the dataset. After that, I kept the musicians as the row names to be able to show them as labels in the plot.

```{r}
scaled.features <- scale(mean_features[-1])
rownames(scaled.features) <- mean_features$Musician
```

I applied the K-Means clustering method, which is one of the most popular techniques of unsupervised statistical learning methods. It is an unsupervised method because is performed on a set of variables X1, X2, Xp with no associated response Y, so there is no outcome to be predicted. It aims to partition n observations into k cluster in which each observation belongs to the cluster with the nearest mean.

Now that I apply the the K-Means algorithm for each musician, I can plot a two-dimensional view of the data. The x-axis and y-axis correspond to the first and second component, and the eigenvectors (represented by red arrows) indicate the directional influence each variable has on the principal components. Let´s have a look at the clusters that result from applying the K-Means algorithm to my dataset.

```{r}
library(ggfortify)
library(ggthemes)
set.seed(5000)

k_means <- kmeans(scaled.features, 5)

kmeans_plot <- autoplot(k_means, 
              main = "K-means clustering", 
              data = scaled.features,
              loadings = TRUE, 
              loadings.colour = "#CC0000",
              loadings.label.colour = "#CC0000", 
              loadings.label = TRUE, 
              loadings.label.size = 2.5,  
              loadings.label.repel=T,
              label.size = 2.5, 
              label.repel = T) + 
  scale_fill_manual(values = c("#000066", "#9999CC", "#66CC99", "#FB7201", "#21CDFF")) + 
  scale_color_manual(values = c("#000066", "#9999CC", "#66CC99", "#FB7201", "#21CDFF")) + 
  theme(plot.title=element_text(size=18, face="bold"))

kmeans_plot
```

I have also plotted another radar chart containing the features for each cluster. It is useful to compare the attributes of the songs that each cluster listen to.

```{r}
library(radarchart)
library(tidyr)
cluster_centers <- as.data.frame(k_means$centers)
cluster <- c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5")
cluster_centers <- cbind(cluster, cluster_centers)
```

```{r}
radarDF_6 <- gather(cluster_centers, key=Attribute, value=Score, -cluster) %>%
  spread(key=cluster, value=Score)
# we change the colours according to clusters
colMatrix = matrix(c(c(4,24,102), c(135,133,193), c(87,196,135), c(251,114,1), c(33,205,255)), nrow = 3)
# chart
chartJSRadar(scores = radarDF_6,
             scaleStartValue = -4, 
             maxScale =1.5, 
             showToolTipLabel = TRUE,
             colMatrix = colMatrix)
```

